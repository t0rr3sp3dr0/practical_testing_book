

<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Flaky test &#8212; The Practical Testing Book</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha384-KA6wR/X5RY4zFAHpv/CnoG2UW1uogYfdnP67Uv7eULvTveboZJg0qUpmJZb5VqzN" crossorigin="anonymous">
    <link href="../_static/css/index.css" rel="stylesheet">
    <link rel="stylesheet" href="../_static/sphinx-book-theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-dropdown.css" />
    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/sphinx-book-theme.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/language_data.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/sphinx-book-theme.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Test Selection and Prioritization" href="selectionprio.html" />
    <link rel="prev" title="Continuous Integration" href="continuousintegration.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="docsearch:language" content="en">



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  <img src="../_static/logo.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">The Practical Testing Book</h1>
  
</a>
</div>

<form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>

<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Introduction
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Fundamentals
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../fundamentals/basics.html">
   Motivation and Basic Definitions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../fundamentals/limitations.html">
   Limitations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../fundamentals/kinds.html">
   The Different Dimensions of Testing
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Test Granularity
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../testgranularity/unittesting.html">
   Unit Testing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../testgranularity/integrationtesting.html">
   Integration Testing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../testgranularity/systemtesting.html">
   System Testing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../testgranularity/acceptancetesting/acceptancetesting.html">
   Acceptance Testing
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Good/Bad Practices
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../goodpractices/artifacts.html">
   Test Artifacts
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../goodpractices/bugreports.html">
   Bug Reports
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Test Adequacy
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../testadequacy/idea.html">
   How Good Are your Tests?
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../testadequacy/coverage.html">
   Structural Testing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../testadequacy/mutation.html">
   Mutation Testing
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Test Generation
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../testgeneration/modelbased.html">
   Model-Based Testing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../testgeneration/combinatorial.html">
   Combinatorial Testing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../testgeneration/random.html">
   Randomized Testing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../testgeneration/fuzzing.html">
   Fuzzing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../testgeneration/propertybased.html">
   Property-based Testing
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Regression Testing
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="buildsystems.html">
   Build Systems
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="continuousintegration.html">
   Continuous Integration
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Flaky test
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="selectionprio.html">
   Test Selection and Prioritization
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Debugging
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../debugging/scientific.html">
   Scientific Debugging
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../debugging/gitbisect.html">
   Git Bisect
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../debugging/deltadebugging.html">
   Hands On: Delta Debugging
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Misc
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../misc/credits.html">
   Credits
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../misc/howtohelp.html">
   How To Help
  </a>
 </li>
</ul>

</nav>

 <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        <div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    
    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/testregression/flakytests.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
    
</div>
        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#example-of-a-flaky-test">
   Example of a flaky test
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#top-causes-for-flaky-tests">
   Top causes for flaky tests
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#identifying-flaky-tests">
   Identifying flaky tests
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#dealing-with-flaky-tests">
   Dealing with flaky tests
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#hands-on">
   Hands On
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#quiz">
   Quiz
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
</ul>

        </nav>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="flaky-test">
<h1>Flaky test<a class="headerlink" href="#flaky-test" title="Permalink to this headline">¶</a></h1>
<p>Unit tests should always have the same results: it will fail or it will pass. In other words, the result should be deterministic. But, sometimes, the same test shows both behaviors, even if there was no change in that unit<a class="bibtex reference internal" href="#deflaker" id="id1">[1]</a>. These non-deterministic tests are known as Flaky Tests and they are more common than you might think, a Google Engineer<a class="bibtex reference internal" href="#googleflakytest" id="id2">[4]</a> says that 16% of their tests have some flakiness.
When the code is passing in the Continuous Integration (CI) system, and a failure occurs, it ends up slowing down or preventing the evolution of the entire pipeline until the fault is found and resolved causing an increase in cost. The problem is that it is difficult to know when a test really failed or when it is Flaky<a class="bibtex reference internal" href="#googleflakytest" id="id3">[4]</a>.</p>
<div class="section" id="example-of-a-flaky-test">
<h2>Example of a flaky test<a class="headerlink" href="#example-of-a-flaky-test" title="Permalink to this headline">¶</a></h2>
<p>Let’s see a practical example of flaky test. Below we have a test with an Async Wait flakiness. It generates a randon number between 1 and 10 and passes this value as a paramater to a sleep function.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">from</span> <span class="nn">threading</span> <span class="kn">import</span> <span class="n">Thread</span>

<span class="k">class</span> <span class="nc">app</span><span class="p">(</span><span class="n">Thread</span><span class="p">):</span>
  <span class="k">def</span> <span class="fm">__init__</span> <span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">Thread</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">res</span> <span class="o">=</span> <span class="mi">0</span>

  <span class="k">def</span> <span class="nf">setResult</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">result</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">res</span> <span class="o">=</span> <span class="n">result</span>

  <span class="k">def</span> <span class="nf">getResult</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">res</span>

  <span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">sleep_length</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">*</span> <span class="mi">10</span>
    <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="n">sleep_length</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">setResult</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
</pre></div>
</div>
<p>To test the code, we’re going to use unittest, a python unit testing framework. In our test we start a thread as defined above and wait 7 seconds to get the result from that thread. If the random sleep time from the thread is higher than 7 seconds, the value of ‘result’ won’t have been set to 100 when the assertEqual function runs and the test will fail, if it’s lower, the test will pass.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">unittest</span>
<span class="kn">import</span> <span class="nn">time</span>

<span class="k">class</span> <span class="nc">AppTestCase</span><span class="p">(</span><span class="n">unittest</span><span class="o">.</span><span class="n">TestCase</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">test_app_function</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
      <span class="n">app_test</span> <span class="o">=</span> <span class="n">app</span><span class="p">()</span>
      <span class="n">app_test</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
      <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mi">7</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">assertEqual</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">app_test</span><span class="o">.</span><span class="n">getResult</span><span class="p">())</span>
</pre></div>
</div>
<p>Now let’s run the test and see what output it gives:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">suite</span> <span class="o">=</span> <span class="n">unittest</span><span class="o">.</span><span class="n">TestLoader</span><span class="p">()</span><span class="o">.</span><span class="n">loadTestsFromTestCase</span><span class="p">(</span><span class="n">AppTestCase</span><span class="p">)</span>
<span class="n">unittest</span><span class="o">.</span><span class="n">TextTestRunner</span><span class="p">()</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">suite</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">F</span>
<span class="o">======================================================================</span>
<span class="n">FAIL</span><span class="p">:</span> <span class="n">test_app_function</span> <span class="p">(</span><span class="n">__main__</span><span class="o">.</span><span class="n">AppTestCase</span><span class="p">)</span>
<span class="o">----------------------------------------------------------------------</span>
<span class="n">Traceback</span> <span class="p">(</span><span class="n">most</span> <span class="n">recent</span> <span class="n">call</span> <span class="n">last</span><span class="p">):</span>
  <span class="n">File</span> <span class="s2">&quot;&lt;ipython-input-51-25320b01c54c&gt;&quot;</span><span class="p">,</span> <span class="n">line</span> <span class="mi">9</span><span class="p">,</span> <span class="ow">in</span> <span class="n">test_app_function</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">assertEqual</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">app_test</span><span class="o">.</span><span class="n">getResult</span><span class="p">())</span>
<span class="ne">AssertionError</span><span class="p">:</span> <span class="mi">100</span> <span class="o">!=</span> <span class="mi">0</span>

<span class="o">----------------------------------------------------------------------</span>
<span class="n">Ran</span> <span class="mi">1</span> <span class="n">test</span> <span class="ow">in</span> <span class="mf">7.006</span><span class="n">s</span>

<span class="n">FAILED</span> <span class="p">(</span><span class="n">failures</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="o">&lt;</span><span class="n">unittest</span><span class="o">.</span><span class="n">runner</span><span class="o">.</span><span class="n">TextTestResult</span> <span class="n">run</span><span class="o">=</span><span class="mi">1</span> <span class="n">errors</span><span class="o">=</span><span class="mi">0</span> <span class="n">failures</span><span class="o">=</span><span class="mi">1</span><span class="o">&gt;</span>
</pre></div>
</div>
<p>The test fails. The sleep time of the thread is higher than the sleep time of the test. Let’s try it again:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">suite</span> <span class="o">=</span> <span class="n">unittest</span><span class="o">.</span><span class="n">TestLoader</span><span class="p">()</span><span class="o">.</span><span class="n">loadTestsFromTestCase</span><span class="p">(</span><span class="n">AppTestCase</span><span class="p">)</span>
<span class="n">unittest</span><span class="o">.</span><span class="n">TextTestRunner</span><span class="p">()</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">suite</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">.</span>
<span class="o">----------------------------------------------------------------------</span>
<span class="n">Ran</span> <span class="mi">1</span> <span class="n">test</span> <span class="ow">in</span> <span class="mf">7.009</span><span class="n">s</span>

<span class="n">OK</span>
<span class="o">&lt;</span><span class="n">unittest</span><span class="o">.</span><span class="n">runner</span><span class="o">.</span><span class="n">TextTestResult</span> <span class="n">run</span><span class="o">=</span><span class="mi">1</span> <span class="n">errors</span><span class="o">=</span><span class="mi">0</span> <span class="n">failures</span><span class="o">=</span><span class="mi">0</span><span class="o">&gt;</span>
</pre></div>
</div>
<p>Now the test passes. The random behavior exhibited by this test is an example of flakiness caused by Async Wait, as we’ll explain next.</p>
</div>
<div class="section" id="top-causes-for-flaky-tests">
<h2>Top causes for flaky tests<a class="headerlink" href="#top-causes-for-flaky-tests" title="Permalink to this headline">¶</a></h2>
<p>According to Luo and colleagues<a class="bibtex reference internal" href="#id15" id="id4">[3]</a> a test can be flaky for several reasons as shown in the graphic bellow:</p>
<p><img alt="" src="../_images/ft_bar.png" /></p>
<p>Async wait, concurrency, test order dependency are the most common types so let’s explain more about them.</p>
<ul class="simple">
<li><p><strong>Async wait</strong>: In an asynchronous wait, sometimes the developer uses a <strong>sleep</strong> function to wait for the end of the execution. If the function finishes before this time, the test passes, if it takes more time, it fails. Many flaky tests caused by the async wait can be fixed using <strong>waitFor</strong><a class="bibtex reference internal" href="#id15" id="id5">[3]</a>. This function, instead of presetting a specific amout of time to wait, bounds to the ocurrence of an action, meaning it waits until a certain action takes place.</p></li>
<li><p><strong>Concurrency</strong>: Just like the async wait problem, other issues related to concurrency also have great impact in causing tests to be flaky. These generally derive from the developer not being mindful of the order in which the operations are being executed by the different threads. This can be settled by adding a synchronization block or making sure the correct execution order of  threads is being obeyed<a class="bibtex reference internal" href="#id15" id="id6">[3]</a>.</p></li>
</ul>
<p><img alt="" src="../_images/concurrency.jpg" /></p>
<p>In the case presented in this figure, the threads are modifying a shared list. When we try to check if an element of the list is equal to a certain value “x”, depending on which thread modified it last the outcome can be different, causing this code to behave non-deterministically.</p>
<ul class="simple">
<li><p><strong>Test Order Dependency</strong>: Sometimes, a test assumes implicit requirements that can’t be complied due to some modification made during the execution of a previous test. In this case, the order in which a set of tests is executed plays a role in influencing the occurrence of a certain output<a class="bibtex reference internal" href="#ifixflakies" id="id7">[5]</a>. For that reason tests should be independent from each other.</p></li>
</ul>
<p><img alt="" src="../_images/orderDependency.jpg" /></p>
<p>As shown in the figure above, when Test 1 for the function isEmpty() is isolated it passes. But when we run Test 2 for insert() before Test 1, Test 1 fails. We added a new value into the list, so it isn’t empty as expected. However, when we run Test 3 for the function remove() after Test 2 and before Test 1, Test 1 passes again. This time, despite having added a new value into the list, we removed it right after, so when Test 1 runs the list is empty again.</p>
</div>
<div class="section" id="identifying-flaky-tests">
<h2>Identifying flaky tests<a class="headerlink" href="#identifying-flaky-tests" title="Permalink to this headline">¶</a></h2>
<p>One way to identify these tests is to re-run the tests several times and mark the tests that show contradictory behaviors as “flaky”. But, it’s hard to determine how many times you need to re-run a test until it proves to be flaky<a class="bibtex reference internal" href="#deflaker" id="id8">[1]</a>. It could still happen that your test exhibited a consistent behavior of failure but it was flaky. What some developers do is to set a threshold for the number of executions after which if the test continuosly gives a failure, they would consider to truly exist a bug in the code<a class="bibtex reference internal" href="#googleflakytest" id="id9">[4]</a>.</p>
<p>There are also tools, like <a class="reference external" href="https://scope.dev/">SCOPE</a><a class="bibtex reference internal" href="#scope" id="id10">[2]</a>, that help to identify these tests in a single run.</p>
<p>The important thing is to identify the flakiness as soon as possible. Establishing a routine where the system is tested several times helps to identify a flaky earlier, reducing the impact on the development of the project.</p>
</div>
<div class="section" id="dealing-with-flaky-tests">
<h2>Dealing with flaky tests<a class="headerlink" href="#dealing-with-flaky-tests" title="Permalink to this headline">¶</a></h2>
<p>Now that we know what a flaky test is and what could cause them, we need to learn how to deal with this type of test.</p>
<p>The approach some teams have to deal with flaky tests is to reject the test that exhibited this behavior, as examining if the issue is with the test or with the code takes time and delays development<a class="bibtex reference internal" href="#googleflakytest" id="id11">[4]</a>. Hence the easiest and most straightforward approach is to assume that the test is incorrect and not the code. However, this can’t be the best alternative, because if there is in fact a bug in the code it can escalate to bigger problems by pushing a broken code ahead.</p>
<p>A safe initial approach is to start tagging tests that are flaky. Beyond that, you’ll need to investigate the reason why a test showed such behavior and to further analyze the impact caused by this issue. In this case, it’s extremely important to collect as much information as possible during the execution of each test: logs, specificities from the environment and memory data from the moment the test was executed, etc<a class="bibtex reference internal" href="#scope" id="id12">[2]</a>. This way it’s easier to reproduce the test that failed and to compare what’s different from the test that passed. As mentioned before, some teams reproduce a failed test countless times, which also helps to evaluate how flaky a test is. Another important piece of information to be considered is when this test started to flake, since it’s usually more complex to find the root problem in tests with older failures.</p>
<p>Once a test is tagged as flaky and data about its execution is collected, you can put this test into quarantine. Its output is disregarded and it shouldn’t be executed in the master pipeline until the issue with it is fixed<a class="bibtex reference internal" href="#scope" id="id13">[2]</a>. Then the assigned developer will start debugging the test, equipped with all the information about in which context this specific test failed and in which it passed. Because most teams set dealing with flaky tests as a high priority, these tests are generally fixed quickly<a class="bibtex reference internal" href="#scope" id="id14">[2]</a>.</p>
</div>
<div class="section" id="hands-on">
<h2>Hands On<a class="headerlink" href="#hands-on" title="Permalink to this headline">¶</a></h2>
<p>You can execute the example we showed earlier in the following notebook. Run the test several times and analyze the different outputs.</p>
<p><a 
href="https://colab.research.google.com/github/damorimRG/practical_testing_book/blob/master/testregression/Flaky_Tests_Hands_On.ipynb" target="_blank">
<img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg"></a></p>
</div>
<div class="section" id="quiz">
<h2>Quiz<a class="headerlink" href="#quiz" title="Permalink to this headline">¶</a></h2>
<p>If you want to test the concepts shown here, try this <a class="reference external" href="https://docs.google.com/forms/d/1mc1ZDXUFzViTQepWg0VxDJfwT_S09ITPSC8o97D2a2k/viewform?edit_requested=true">quiz</a>.</p>
</div>
<div class="section" id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<p id="bibtex-bibliography-testregression/flakytests-0"><dl class="citation">
<dt class="bibtex label" id="deflaker"><span class="brackets">1</span><span class="fn-backref">(<a href="#id1">1</a>,<a href="#id8">2</a>)</span></dt>
<dd><p>Jonathan Bell, Owolabi Legunsen, Michael Hilton, Lamyaa Eloussi, Tifany Yung, and Darko Marinov. Deflaker: automatically detecting flaky tests. In <em>Proceedings of the 40th International Conference on Software Engineering</em>, ICSE ’18, 433–444. New York, NY, USA, 2018. Association for Computing Machinery. URL: <a class="reference external" href="https://doi.org/10.1145/3180155.3180164">https://doi.org/10.1145/3180155.3180164</a>, <a class="reference external" href="https://doi.org/10.1145/3180155.3180164">doi:10.1145/3180155.3180164</a>.</p>
</dd>
<dt class="bibtex label" id="scope"><span class="brackets">2</span><span class="fn-backref">(<a href="#id10">1</a>,<a href="#id12">2</a>,<a href="#id13">3</a>,<a href="#id14">4</a>)</span></dt>
<dd><p>Bryan Lee. How can we peacefully co-exist with flaky tests? Apr 2020. Accessed: 2020-07-27. URL: <a class="reference external" href="https://medium.com/scopedev/how-can-we-peacefully-co-exist-with-flaky-tests-3c8f94fba166">https://medium.com/scopedev/how-can-we-peacefully-co-exist-with-flaky-tests-3c8f94fba166</a>.</p>
</dd>
<dt class="bibtex label" id="id15"><span class="brackets">3</span><span class="fn-backref">(<a href="#id4">1</a>,<a href="#id5">2</a>,<a href="#id6">3</a>)</span></dt>
<dd><p>Qingzhou Luo, Farah Hariri, Lamyaa Eloussi, and Darko Marinov. An empirical analysis of flaky tests. In <em>Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering</em>, FSE 2014, 643–653. New York, NY, USA, 2014. Association for Computing Machinery. URL: <a class="reference external" href="https://doi.org/10.1145/2635868.2635920">https://doi.org/10.1145/2635868.2635920</a>, <a class="reference external" href="https://doi.org/10.1145/2635868.2635920">doi:10.1145/2635868.2635920</a>.</p>
</dd>
<dt class="bibtex label" id="googleflakytest"><span class="brackets">4</span><span class="fn-backref">(<a href="#id2">1</a>,<a href="#id3">2</a>,<a href="#id9">3</a>,<a href="#id11">4</a>)</span></dt>
<dd><p>John Micco. Flaky tests at google and how we mitigate them. May 2016. Accessed: 2020-07-27. URL: <a class="reference external" href="https://testing.googleblog.com/2016/05/flaky-tests-at-google-and-how-we.html">https://testing.googleblog.com/2016/05/flaky-tests-at-google-and-how-we.html</a>.</p>
</dd>
<dt class="bibtex label" id="ifixflakies"><span class="brackets"><a class="fn-backref" href="#id7">5</a></span></dt>
<dd><p>August Shi, Wing Lam, Reed Oei, Tao Xie, and Darko Marinov. Ifixflakies: a framework for automatically fixing order-dependent flaky tests. In <em>Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering</em>, ESEC/FSE 2019, 545–555. New York, NY, USA, 2019. Association for Computing Machinery. URL: <a class="reference external" href="https://doi.org/10.1145/3338906.3338925">https://doi.org/10.1145/3338906.3338925</a>, <a class="reference external" href="https://doi.org/10.1145/3338906.3338925">doi:10.1145/3338906.3338925</a>.</p>
</dd>
</dl>
</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./testregression"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="continuousintegration.html" title="previous page">Continuous Integration</a>
    <a class='right-next' id="next-link" href="selectionprio.html" title="next page">Test Selection and Prioritization</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By The World<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    <script src="../_static/js/index.js"></script>
    
  </body>
</html>